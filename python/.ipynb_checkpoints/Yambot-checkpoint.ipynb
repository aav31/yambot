{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b226652d",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=IS0V8z8HXrM&list=PL-9x0_FO_lgkwi8ES611NsV-cjYaH_nLa&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281e1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from YambEnv import ROW, COL, YambEnv, Action\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79ebbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we need to be able to enumerate all the arrays of size 6 we can have which sum to 5\n",
    "# Generalize by n different buckets throw in 5 balls\n",
    "@lru_cache(maxsize=10)\n",
    "def dp(n, k):\n",
    "    result = set()\n",
    "    if k==0:\n",
    "        result.add((0,)*n)\n",
    "        return result\n",
    "    \n",
    "    recursive_result = dp(n, k-1)\n",
    "    for tup in recursive_result:\n",
    "        for i in range(n):\n",
    "            new_arr = list(tup)\n",
    "            new_arr[i] += 1\n",
    "            result.add(tuple(new_arr))\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3de60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all possible count arrays of dice we can keep\n",
    "COUNT_ARRAYS = list(reversed(sorted(list(dp(6, 5)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "877ea8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, agent_type : int):\n",
    "        assert 1 <= agent_type <= 3, \"Agent must be of type 1, 2 or 3\"\n",
    "        self.agent_type = agent_type\n",
    "        self.discount_rate = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "        self.input_dim, self.output_dim = self._get_dims()\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy, self.predict = self._build_policy_network()\n",
    "        self.action_space = [i for i in range(self.output_dim)]\n",
    "        self.model_file = 'agent_{}'.format(agent_type)\n",
    "        \n",
    "    def _get_dims(self):\n",
    "        \"\"\"Get the dimensions of the input to the network and the output to the network\n",
    "        :return: input_dim, output_dim\n",
    "        \"\"\"\n",
    "        grid_dim = 14 * 4\n",
    "        announced_dim = 15 # first 14 tell us which row we announced, 15th tells us we did not announce\n",
    "        roll_input_dim = len(COUNT_ARRAYS)\n",
    "        roll_output_dim = len(COUNT_ARRAYS)  # will give an array of probabilities. each is mapped to different count array\n",
    "        \n",
    "        if self.agent_type == 1:\n",
    "            return grid_dim + roll_input_dim, roll_output_dim * announced_dim\n",
    "        \n",
    "        if self.agent_type == 2:\n",
    "            return grid_dim + roll_input_dim + announced_dim, roll_output_dim\n",
    "        \n",
    "        if self.agent_type == 3:\n",
    "            return grid_dim + roll_input_dim + announced_dim, grid_dim\n",
    "        \n",
    "    def _convert_observation_to_input(self, observation : dict):\n",
    "        \"\"\"\n",
    "        :param observation: dictionary which comes from the environment\n",
    "        :return: numpy array in the appropriate format to be consumed by the model as an input\n",
    "        \"\"\"\n",
    "        assert self.agent_type == observation[\"roll_number\"], \"Agent type should match roll number\"\n",
    "        grid = np.nan_to_num(observation[\"grid\"].flatten() / 100, nan=-1)\n",
    "        roll = np.eye(len(COUNT_ARRAYS))[COUNT_ARRAYS.index( tuple(observation[\"roll\"]) )]\n",
    "        announced = np.eye(15)[observation[\"announced_row\"] if observation[\"announced\"] else 14]\n",
    "        \n",
    "        if self.agent_type == 1:\n",
    "            return np.hstack([grid, roll])\n",
    "        \n",
    "        if self.agent_type == 2:\n",
    "            return np.hstack([grid, roll, announced])\n",
    "        \n",
    "        if self.agent_type == 3:\n",
    "            return np.hstack([grid, roll, announced])\n",
    "        \n",
    "    def _convert_action_to_output(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _build_policy_network(self):\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        dense = Dense(100, activation='relu')(inputs)\n",
    "        probs = Dense(self.output_dim, activation='softmax')(dense)\n",
    "        # need to understand how to setup model so that it can do policy gradient\n",
    "        policy = Model(inputs=inputs, outputs=probs, name='policy_network_1')\n",
    "        policy.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=SparseCategoricalCrossentropy(from_logits=True))\n",
    "        predict = Model(inputs=inputs, outputs=probs)\n",
    "        return policy, predict\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        :param observation: observation from the environment -> needs to convert to correct format before network uses\n",
    "        :return: needs to return an Action\n",
    "        \"\"\"\n",
    "        state = self._convert_to_\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        action = self._convert_action_to_output()\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        actions = np.zeros([len(action_memory), self.n_actions])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G - mean) / std\n",
    "\n",
    "        cost = self.policy.train_on_batch([state_memory, self.G], actions)\n",
    "\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05013f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = YambEnv()\n",
    "# n_episodes = 1\n",
    "# for i in range(n_episodes):\n",
    "#     observation = env.reset()\n",
    "#     truncated, terminated = False, False\n",
    "#     score = 0\n",
    "#     while not(terminated or truncated):\n",
    "#         action = agent.choose_action(observation)\n",
    "#         observation_new, reward, terminated, truncated, truncation_reason = env.step(action)\n",
    "#         agent.store_transition(observation, action, reward)\n",
    "#         observation = observation_new\n",
    "#         score += reward\n",
    "        \n",
    "#     score_history.append(score)\n",
    "#     agent.learn()\n",
    "#     print(\"Episode: {}, Score: {}, Average score: {}\".format(i, score, sum(score_history[-100:]) / 100.0))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1082d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = YambEnv()\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88b8d8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8c0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
