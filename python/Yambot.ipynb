{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b226652d",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=IS0V8z8HXrM&list=PL-9x0_FO_lgkwi8ES611NsV-cjYaH_nLa&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281e1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from YambEnv import ROW, COL, YambEnv, Action\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877ea8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99, n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "\n",
    "        self.model_file = fname\n",
    "\n",
    "    def build_policy_network(self):\n",
    "        input = Input(shape=(self.input_dims,))\n",
    "        advantages = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(input)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "\n",
    "            return K.sum(-log_lik*advantages)\n",
    "\n",
    "        policy = Model(input=[input, advantages], output=[probs])\n",
    "\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "\n",
    "        predict = Model(input=[input], output=[probs])\n",
    "\n",
    "        return policy, predict\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        actions = np.zeros([len(action_memory), self.n_actions])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G - mean) / std\n",
    "\n",
    "        cost = self.policy.train_on_batch([state_memory, self.G], actions)\n",
    "\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05013f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = YambEnv()\n",
    "n_episodes = 1\n",
    "for i in range(n_episodes):\n",
    "    observation = env.reset()\n",
    "    truncated, terminated = False, False\n",
    "    score = 0\n",
    "    while not(terminated or truncated):\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_new, reward, terminated, truncated, truncation_reason = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_new\n",
    "        score += reward\n",
    "        \n",
    "    score_history.append(score)\n",
    "    agent.learn()\n",
    "    print(\"Episode: {}, Score: {}, Average score: {}\".format(i, score, sum(score_history[-100:]) / 100.0))\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
